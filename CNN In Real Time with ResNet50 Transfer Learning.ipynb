{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ee7a55",
   "metadata": {},
   "source": [
    "## **CNN in Real Time using Transfer Learning with ResNet50 on CIFAR10 Dataset** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf312d0c",
   "metadata": {},
   "source": [
    "\n",
    "This app recognize movments, detect the object and tracking after it in real time from camera.                                   \n",
    "We use **Resnet50** model with **Transfer Learning** on **CIFAR10** Dataset that we trained and perfoms **85% accuracy.**       \n",
    "After we recognize movment we detect the object by using our model and then we start tracking after it.                         \n",
    "We store all the detections events in format of date and time and and show it on the screen.                                     \n",
    "The application using **TensorFlow** with **Keras** by **Python**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00e8ce",
   "metadata": {},
   "source": [
    "### Import Libraries, Loading model and Camera  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0226d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " model (Functional)          (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 100352)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               25690368  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,348,490\n",
      "Trainable params: 25,759,754\n",
      "Non-trainable params: 23,588,736\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import cv2\n",
    "import h5py\n",
    "import winsound\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import strftime\n",
    "from collections import deque\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the labels of CIFAR-10 datasest\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Load our model that trained by 25 epochs on CIFAR dataset\n",
    "model = load_model(r'C:\\\\Project CNN In Real Time\\cifar10_ResNet50_88_Accuracy.h5')\n",
    "\n",
    "# Catch frame from webcam\n",
    "camera = cv2.VideoCapture(r'C:\\\\Project CNN In Real Time\\Videos\\Pigeons\\Pigeon 6.mp4')\n",
    "# camera = cv2.VideoCapture(1)\n",
    "\n",
    "# This path is loaction for the saved images \n",
    "outPutPath = (r'C:\\\\Project CNN In Real Time\\Saved Images\\Detections Events') \n",
    "\n",
    "# This path is location for the saved detections events\n",
    "txtPath = (r'C:\\\\Project CNN In Real Time\\Saved Images\\Detections Events\\Detection Events.txt')\n",
    "\n",
    "# This path is location for the sound file\n",
    "soundPath = (r'C:\\\\Project CNN In Real Time\\Videos\\Sounds\\Notification 2.wav') \n",
    "\n",
    "# Writing The Headline of the text file\n",
    "with open(txtPath, 'a') as f:\n",
    "    f.write(\"###################### Detection Events #########################\")\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "        \n",
    "# Print trained model summery\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961f58b2",
   "metadata": {},
   "source": [
    "### Define The Application Constans Variabels ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820885de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define object length variables\n",
    "MIN_OBJECT_AREA = 1000\n",
    "MAX_OBJECT_AREA = 8000\n",
    "\n",
    "# Define the mask by number from 0 to 7\n",
    "mask_number = 6\n",
    "\n",
    "# Check if fragment still in tracking boundries every X frames\n",
    "tracking_check = 10000\n",
    "\n",
    "# Define variables for hight and width shape to prediction model input\n",
    "hight, width = 224, 224\n",
    "\n",
    "# Define variables for hight and width shape of the frames\n",
    "HEIGH, WIDTH = 250, 400   \n",
    "\n",
    "# Define the font size as precent from the screen size\n",
    "FONT_SIZE = ((HEIGH/1000))\n",
    "\n",
    "# Variables for start rows and cols to put text\n",
    "FIRST_ROW = int(HEIGH/10) +2\n",
    "FIRST_COL = int(WIDTH/40) +2\n",
    "ROWS_SPACE = int(HEIGH/10) -5\n",
    "\n",
    "# Initializing deque object for center points of the detected object\n",
    "points = deque(maxlen=50)\n",
    "\n",
    "# Create backgroung of the main frame\n",
    "foregroundModel = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Sobel filter to detect vertical changes on image\n",
    "f1 = np.array([[1, 0, -1],\n",
    "               [2, 0, -2],\n",
    "               [1, 0, -1]])\n",
    "\n",
    "# Initializing Conv2D layer for GRAY input\n",
    "layer = tf.keras.layers.Conv2D(filters=1,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               activation='relu',\n",
    "                               input_shape=(HEIGH, WIDTH, 1),\n",
    "                               use_bias=False,\n",
    "                               kernel_initializer=tf.keras.initializers.constant(f1))\n",
    "\n",
    "\n",
    "# Define tracker dictionary\n",
    "tracker_dict = { 'csrt': cv2.TrackerCSRT_create,\n",
    "                 'goturn': cv2.TrackerGOTURN_create,\n",
    "                 'kcf' : cv2.TrackerKCF_create,\n",
    "                 'boosting' : cv2.legacy.TrackerMOSSE_create(),\n",
    "                 'mil': cv2.TrackerMIL_create,\n",
    "                 'tld': cv2.legacy.TrackerTLD_create(),\n",
    "                 'medianflow': cv2.legacy.TrackerMedianFlow_create(),\n",
    "                 'mosse':cv2.legacy.TrackerMOSSE_create()}\n",
    "\n",
    "\n",
    "# Initialize our tracker after the object\n",
    "tracker = tracker_dict['csrt']()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce29b20",
   "metadata": {},
   "source": [
    "### Define number of masks for different kind of tasks ###\n",
    "Masks using to recognize movements in frames and detect object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2dd6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function return the right mask according to mask_number\n",
    "def mask(frame, last_frame, foreground_frame, mask_number):\n",
    "\n",
    "    # Create 7 different types of masks to the main frame\n",
    "    if mask_number == 0:\n",
    "        mask_frame = absdiff_mask(frame_bgr, last_frame)      \n",
    "    if mask_number == 1: \n",
    "        mask_frame = mask1(frame, foreground_mask) # 15 birds\n",
    "    if mask_number == 2:  \n",
    "        mask_frame = mask2(frame, foreground_mask) # 25 birds\n",
    "    if mask_number == 3:\n",
    "        mask_frame = mask3(frame, foreground_mask) # 10 birds - good tracking\n",
    "    if mask_number == 4:\n",
    "        mask_frame = mask4(frame, last_frame) # 16 birds - good tracking\n",
    "    if mask_number == 5:\n",
    "        mask_frame = mask5(frame, last_frame) # 7 birds\n",
    "    if mask_number == 6:\n",
    "        mask_frame = mask6(frame, foreground_mask) # 46 birds\n",
    "    if mask_number == 7:\n",
    "        mask_frame = mask7(frame, foreground_mask) # 4 birds\n",
    "\n",
    "    return mask_frame\n",
    "\n",
    "# Function create a mask with and Treshold\n",
    "def absdiff_mask(frame_bgr, last_frame):\n",
    "      \n",
    "    # Slicing from tuple only first two elements\n",
    "    (high, width) = frame_bgr.shape[:2]\n",
    "\n",
    "    # Converting captured frame to GRAY by OpenCV function  \n",
    "    frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create one more frame with Gaussian blur\n",
    "    frame_gray = cv2.GaussianBlur(frame_gray, (25, 25), 0)  \n",
    "    \n",
    "    # Converting captured frame to GRAY by OpenCV function        \n",
    "    last_frame = cv2.cvtColor(last_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Create one more frame with Gaussian blur\n",
    "    last_frame = cv2.GaussianBlur(last_frame, (25, 25), 0)  \n",
    "    \n",
    "    # Calculate the difference between two frames\n",
    "    abs_diff = cv2.absdiff(last_frame, frame_gray)\n",
    "    \n",
    "    # Function exclude values that ara more than treshhold = 15 0 and more than 255\n",
    "    _, gray_mask = cv2.threshold(abs_diff, 5, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    return gray_mask\n",
    "\n",
    "# Function create a mask with Sobel filter to detect vertical changes on image\n",
    "def mask1(frame_bgr, foreground_mask):\n",
    "\n",
    "    # Slicing from tuple only first two elements\n",
    "    (high, width) = frame_bgr.shape[:2]\n",
    "\n",
    "    # Converting captured frame to GRAY by OpenCV function    \n",
    "    frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Reshaping frame to (batch size, rows, columns, channels)\n",
    "    input_frame = frame_gray.reshape(1, high, width, 1).astype(np.float32)\n",
    "    \n",
    "    # Passing GRAY input to the Conv2D layer and to find contours later\n",
    "    mask = layer(input_frame)\n",
    "    \n",
    "    # Converting output feature map from Tensor to Numpy array\n",
    "    mask_frame = np.array(mask[0, :, :, 0])\n",
    "\n",
    "    # Using 'clip' function to exclude values that are less than 0 and more than 255\n",
    "    mask_frame = np.clip(mask_frame, 0, 255).astype(np.uint8) \n",
    "        \n",
    "    return mask_frame\n",
    "\n",
    "# Function create a mask with Sobel filter and GaussianBlur\n",
    "def mask2(frame_bgr, foreground_mask):\n",
    "\n",
    "    # Slicing from tuple only first two elements\n",
    "    (high, width) = frame_bgr.shape[:2]\n",
    "\n",
    "    # Converting captured frame to GRAY by OpenCV function    \n",
    "    frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create one more frame with Gaussian blur\n",
    "    frame_gray = cv2.GaussianBlur(frame_gray, (25, 25), 0)  \n",
    "    \n",
    "    # Reshaping frame to (batch size, rows, columns, channels)\n",
    "    input_frame = frame_gray.reshape(1, high, width, 1).astype(np.float32)\n",
    "    \n",
    "    # Passing GRAY input to the Conv2D layer and to find contours later\n",
    "    mask = layer(input_frame)\n",
    "    \n",
    "    # Converting output feature map from Tensor to Numpy array\n",
    "    mask_frame = np.array(mask[0, :, :, 0])\n",
    "\n",
    "    # Using 'clip' function to exclude values that are less than 0 and more than 255\n",
    "    mask_frame = np.clip(mask_frame, 0, 255).astype(np.uint8) \n",
    "        \n",
    "    return mask_frame\n",
    "\n",
    "# Function create a mask with Sobel filter and Treshold\n",
    "def mask3(frame_bgr, foreground_mask):\n",
    "    \n",
    "    # Slicing from tuple only first two elements\n",
    "    (high, width) = frame_bgr.shape[:2]\n",
    "\n",
    "    # Converting captured frame to GRAY by OpenCV function    \n",
    "    gray_frame = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create one more frame with Gaussian blur\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (25, 25), 0)  \n",
    "    \n",
    "    # Reshaping frame to (batch size, rows, columns, channels)\n",
    "    gray_frame = gray_frame.reshape(1, high, width, 1).astype(np.float32)\n",
    "\n",
    "    # Passing GRAY input to the Conv2D layer and to find contours later    \n",
    "    mask = layer(gray_frame)\n",
    "    \n",
    "    # Converting output feature map from Tensor to Numpy array\n",
    "    mask_frame = np.array(mask[0, :, :, 0])\n",
    "\n",
    "    # Using 'clip' function to exclude values that are less than 0 and more than 255\n",
    "    mask_frame = np.clip(mask_frame, 0, 255).astype(np.uint8)  \n",
    "    \n",
    "    # Function exclude values that ara more than treshhold = 15 0 and more than 255\n",
    "    _, mask_frame = cv2.threshold(mask_frame, 8, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    return mask_frame\n",
    "\n",
    "# Function create a mask with and Treshold\n",
    "def mask4(frame_bgr, last_frame):\n",
    "      \n",
    "    # Slicing from tuple only first two elements\n",
    "    (high, width) = frame_bgr.shape[:2]\n",
    "\n",
    "    # Converting captured frame to GRAY by OpenCV function        \n",
    "    frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create one more frame with Gaussian blur\n",
    "    frame_gray = cv2.GaussianBlur(frame_gray, (25, 25), 0)  \n",
    "\n",
    "    # Converting captured frame to GRAY by OpenCV function        \n",
    "    last_frame = cv2.cvtColor(last_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create one more frame with Gaussian blur\n",
    "    last_frame = cv2.GaussianBlur(last_frame, (25, 25), 0)    \n",
    "\n",
    "    # Return mask to detect change between two frames   \n",
    "    abs_diff = cv2.absdiff(last_frame, frame_gray)\n",
    "    \n",
    "    # Function exclude values that ara more than treshhold = 15 0 and more than 255\n",
    "    _, mask_frame = cv2.threshold(abs_diff, 5, 255, cv2.THRESH_BINARY)    \n",
    "    \n",
    "    return mask_frame\n",
    "\n",
    "# Function create a mask with Sobel filter and Treshold\n",
    "def mask5(frame_bgr, last_frame):\n",
    "      \n",
    "    # Slicing from tuple only first two elements\n",
    "    (high, width) = frame_bgr.shape[:2]\n",
    "    \n",
    "    # Converting captured frame to GRAY by OpenCV function            \n",
    "    frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create one more frame with Gaussian blur\n",
    "    frame_gray = cv2.GaussianBlur(frame_gray, (25, 25), 0)  \n",
    "\n",
    "    # Converting captured frame to GRAY by OpenCV function        \n",
    "    last_frame = cv2.cvtColor(last_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Create one more frame with Gaussian blur\n",
    "    last_frame = cv2.GaussianBlur(last_frame, (25, 25), 0)    \n",
    "    \n",
    "    # Return mask to detect change between two frames\n",
    "    abs_diff = cv2.absdiff(last_frame, frame_gray)\n",
    "\n",
    "    # Reshaping frame to (batch size, rows, columns, channels)    \n",
    "    gray_frame = abs_diff.reshape(1, high, width, 1).astype(np.float32)\n",
    "    \n",
    "    # Passing GRAY input to the Conv2D layer and to find contours later       \n",
    "    mask = layer(gray_frame)\n",
    "    \n",
    "    # Converting output feature map from Tensor to Numpy array\n",
    "    mask_frame = np.array(mask[0, :, :, 0])\n",
    "\n",
    "    # Using 'clip' function to exclude values that are less than 0 and more than 255    \n",
    "    mask_frame = np.clip(mask_frame, 0, 255).astype(np.uint8)  \n",
    "    \n",
    "    # Function exclude values that ara more than treshhold = 15 0 and more than 255\n",
    "    _, mask_frame = cv2.threshold(mask_frame, 70, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    return mask_frame\n",
    "\n",
    "# Function create a mask with connectedComponents\n",
    "def mask6(frame_bgr, foreground_mask):\n",
    "    \n",
    "    # Reduce noises\n",
    "    structuring_element = cv2.getStructuringElement(cv2.MORPH_RECT,(3,3))\n",
    "    foreground_mask = cv2.morphologyEx(np.float32(foreground_mask), cv2.MORPH_OPEN, structuring_element)\n",
    "\n",
    "    # Find out connected components and keep only the large components\n",
    "    num_labels, image_labels = cv2.connectedComponents(np.array(0<foreground_mask, np.uint8))\n",
    "    \n",
    "    # Return components larger than threshold\n",
    "    foreground_mask = keepLargeComponents(image_labels, treshold=1000) \n",
    "    \n",
    "    # Using 'clip' function to exclude values that are less than 0 and more than 255\n",
    "    foreground_mask = np.clip(foreground_mask, 0, 255).astype(np.uint8) \n",
    "    \n",
    "    # Converting output feature map from Tensor to Numpy array\n",
    "    foreground_mask = foreground_mask[:, :, np.newaxis]  \n",
    "    \n",
    "    return foreground_mask\n",
    "\n",
    "# Function create a mask with Sobel filter and connectedComponents\n",
    "def mask7(frame_bgr, foreground_mask):\n",
    "    \n",
    "    # Reduce noises\n",
    "    structuring_element = cv2.getStructuringElement(cv2.MORPH_RECT,(3,3))\n",
    "    foreground_mask = cv2.morphologyEx(np.float32(foreground_mask), cv2.MORPH_OPEN, structuring_element)\n",
    "\n",
    "    # Find out connected components and keep only the large components\n",
    "    num_labels, image_labels = cv2.connectedComponents(np.array(0<foreground_mask, np.uint8))\n",
    "    \n",
    "    # Return components larger than threshold\n",
    "    foreground_mask = keepLargeComponents(image_labels, treshold=0) \n",
    "    \n",
    "    # Using 'clip' function to exclude values that are less than 0 and more than 255\n",
    "    foreground_mask = np.clip(foreground_mask, 0, 255).astype(np.uint8) \n",
    "    \n",
    "    # Function exclude values that ara more than treshhold = 15 0 and more than 255\n",
    "    _, foreground_mask = cv2.threshold(foreground_mask, 0, 255, cv2.THRESH_BINARY)     \n",
    "    \n",
    "    # Converting output feature map from Tensor to Numpy array\n",
    "    foreground_mask = foreground_mask[:, :, np.newaxis]  \n",
    "        \n",
    "    return foreground_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7fca5c",
   "metadata": {},
   "source": [
    "### Define The Application Functions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd68c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calculate the area around the object\n",
    "def cont_area(contour, index):\n",
    "    \n",
    "    # Get an approximate rectangle coordinates\n",
    "    (x_min, y_min, box_width, box_height) = cv2.boundingRect(contours[index])\n",
    "    \n",
    "    # Calculate the area of the rectangle\n",
    "    area = box_width*box_height\n",
    "        \n",
    "    return area\n",
    "\n",
    "# Function return empty frame for initlize the main window\n",
    "def empty_frame(frame_bgr):\n",
    "    \n",
    "    empy_frame = np.zeros(frame_bgr.shape, np.uint8)\n",
    "    empy_frame[:, :, 0] = 255\n",
    "    empy_frame[:, :, 1] = 255\n",
    "    empy_frame[:, :, 2] = 255\n",
    "    \n",
    "    return empy_frame\n",
    "\n",
    "# Function return copy of frame\n",
    "def duplicate_frame(frame):\n",
    "    \n",
    "    duplicate = np.zeros(frame.shape, np.uint8)\n",
    "    duplicate[:, :, 0] = frame[:, :, 0]\n",
    "    duplicate[:, :, 1] = frame[:, :, 1]\n",
    "    duplicate[:, :, 2] = frame[:, :, 2]\n",
    "    \n",
    "    return duplicate\n",
    "\n",
    "# Function return 3-Dimension frame\n",
    "def expands_dimensions(frame):\n",
    "    \n",
    "    new_image = np.zeros((HEIGH, WIDTH, 3), np.uint8)\n",
    "    new_image[:, :, 0] = frame[:, :, 0]\n",
    "    new_image[:, :, 1] = frame[:, :, 0]\n",
    "    new_image[:, :, 2] = frame[:, :, 0]    \n",
    "    \n",
    "    return new_image\n",
    "            \n",
    "# Function checks if the coordinets into the frame boundries\n",
    "def check_rect_coord(x_min, y_min, box_width, box_height):\n",
    "    \n",
    "    # Check if coordinets in the frame boundries\n",
    "    if 0 < x_min and x_min + box_width < WIDTH and 0 < y_min and y_min + box_height < HEIGH:\n",
    "        if 0 < box_width and box_width < WIDTH and 0 < box_height and box_height < HEIGH:\n",
    "\n",
    "            # Rectangle is in the frame \n",
    "            return True\n",
    "        \n",
    "    # Rectangle is not in the frame        \n",
    "    return False\n",
    "\n",
    "# Function return how many mitnutes passed between two detections\n",
    "def ditections_timer(detection_time, last_time):\n",
    "                    \n",
    "    element = last_time\n",
    "    minute = element[len(element) -2]\n",
    "    minute = int(minute) * 10\n",
    "    second = element[len(element) -1]\n",
    "    second = int(second)\n",
    "    minutes = minute + second\n",
    "    last_time_minutes = minutes\n",
    "\n",
    "    element = detection_time\n",
    "    minute = element[len(element) -2]\n",
    "    minute = int(minute) * 10\n",
    "    second = element[len(element) -1]\n",
    "    second = int(second)\n",
    "    minutes = minute + second\n",
    "    detection_time_minutes = minutes\n",
    "\n",
    "    time_past = abs(detection_time_minutes - last_time_minutes)\n",
    "    \n",
    "    return time_past\n",
    "\n",
    "# This function remove the components that are smaller than praticular threshold\n",
    "def keepLargeComponents(image, treshold):\n",
    "    \n",
    "    frame = np.zeros(image.shape) < 0 # boolean array\n",
    "    unique_labels = np.unique(image.flatten()) # find out every unique value that is actually a label \n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label == 0: # background\n",
    "            pass\n",
    "        else:\n",
    "            img = (image == label) # save the component\n",
    "            if treshold < np.sum(img):\n",
    "                frame = frame | img # save all the components\n",
    "                \n",
    "    return np.float32(255*frame)\n",
    "\n",
    "# Function plot bar chart with scores values\n",
    "def bar_chart(obtained_scores, classes_names):\n",
    "    \n",
    "    # Arranging X axis\n",
    "    x_positions = np.arange(obtained_scores.size)\n",
    "\n",
    "    # Creating bar chart\n",
    "    bars = plt.bar(x_positions, obtained_scores, align='center', alpha=0.6)\n",
    "\n",
    "    # Highlighting the highest bar\n",
    "    bars[np.argmax(obtained_scores)].set_color('red')\n",
    "\n",
    "    # Giving labels to bars along X axis\n",
    "    plt.xticks(x_positions, classes_names, rotation=25, fontsize=10)\n",
    "\n",
    "    # Giving names to axes\n",
    "    plt.xlabel('Class', fontsize=20)\n",
    "    plt.ylabel('Value', fontsize=20)\n",
    "\n",
    "    # Giving name to bar chart\n",
    "    plt.title('Obtained Scores', fontsize=20)\n",
    "\n",
    "    # Adjusting borders of the plot\n",
    "    plt.tight_layout(pad=2.5)\n",
    "\n",
    "    # Initializing object of the buffer\n",
    "    b = io.BytesIO()\n",
    "\n",
    "    # Saving bar chart into the buffer\n",
    "    plt.savefig(b, format='png', dpi=200)\n",
    "\n",
    "    # Closing plot with bar chart\n",
    "    plt.close()\n",
    "\n",
    "    # Moving pointer to the beginning of the buffer\n",
    "    b.seek(0)\n",
    "\n",
    "    # Reading bar chart from the buffer\n",
    "    bar_image = np.frombuffer(b.getvalue(), dtype=np.uint8)\n",
    "\n",
    "    # Closing buffer\n",
    "    b.close()\n",
    "\n",
    "    # Decoding buffer\n",
    "    bar_image = cv2.imdecode(bar_image, 1)\n",
    "\n",
    "    # Resize frame to HEIGH X WIDTH\n",
    "    bar_image = cv2.resize(bar_image, (WIDTH, HEIGH))\n",
    "\n",
    "    # Returning Numpy array with bar chart\n",
    "    return bar_image\n",
    "\n",
    "# Function cut the detected fragment and return it\n",
    "def cut_fragment(frame_bgr, contours, index):\n",
    "    \n",
    "    # Get an approximate rectangle coordinates\n",
    "    (x_min, y_min, box_width, box_height) = cv2.boundingRect(contours[index])\n",
    "    \n",
    "    # Cutting detected fragment from BGR frame\n",
    "    cut_fragment_bgr_frame = frame_bgr[y_min: y_min + box_height, x_min: x_min + box_width]\n",
    "    \n",
    "    # Resize the fragment to the right frame shape in the main window\n",
    "    cut_fragment_bgr_frame = cv2.resize(cut_fragment_bgr_frame, (WIDTH, HEIGH))\n",
    "\n",
    "    return cut_fragment_bgr_frame\n",
    "        \n",
    "# Function predict model's output from the cutted fragment    \n",
    "def prediction_model(cut_fragment_bgr):\n",
    "    \n",
    "    # Create a copy of the cut_fragment_bgr frame  \n",
    "    fragment = duplicate_frame(cut_fragment_bgr)\n",
    "\n",
    "    # Swapping channels from BGR to RGB by OpenCV function\n",
    "    fragment = cv2.cvtColor(fragment, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Resizing frame to the right shape of the model's input\n",
    "    fragment = cv2.resize(fragment, (width, hight), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    # Extending dimension from (height, width, channels) to (1, heigh, width, channels)\n",
    "    fragment = fragment[np.newaxis, :, :, :]\n",
    "\n",
    "    # Predict score from model\n",
    "    scores = model.predict(fragment)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Function drawing rectangle around the predicted object\n",
    "def drawing_rectangle(frame_bgr, label, contours, index):\n",
    "    \n",
    "    # Get an approximate rectangle coordinates\n",
    "    (x_min, y_min, box_width, box_height) = cv2.boundingRect(contours[index])\n",
    "\n",
    "    # Drawing bounding box on the current BGR frame\n",
    "    cv2.rectangle(frame_bgr, (x_min, y_min), (x_min + box_width, y_min + box_height), (0, 255, 0), 3)\n",
    "\n",
    "    # Putting text with label on the current BGR frame\n",
    "    cv2.putText(frame_bgr, label, (x_min - 5, y_min - 25), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n",
    "\n",
    "    return frame_bgr   \n",
    "\n",
    "# Function save the frame of the detected event\n",
    "def save_detection_event(frame_bgr, detection_time, label):\n",
    "\n",
    "    # Create a copy of time object\n",
    "    timer = detection_time\n",
    "    \n",
    "    # Save detection_time in format that fit to files\n",
    "    timer = timer[:2] + \"-\" + timer[3:5] + \"-\" + timer[6:13] + \"-\" + timer[14:16] + \"-\" + timer[17:]\n",
    "\n",
    "    # Define the name of the image\n",
    "    image_name = label + ' ' + str(timer)+'.jpg'\n",
    "    \n",
    "    # Define the file adress\n",
    "    finalPath = os.path.join(outPutPath, image_name)\n",
    "    \n",
    "    # Save the frame of detection event\n",
    "    cv2.imwrite(finalPath, frame_bgr)\n",
    "\n",
    "    # Checking if there is detection\n",
    "    if 0 < len(label): \n",
    "        \n",
    "        # Order all line to the same length\n",
    "        if len(label) < len(\"airplane\"):\n",
    "            label = str(label) + (len(\"airplane\") - len(label))*\" \"\n",
    "            \n",
    "        # Create string of detection event in date-time format\n",
    "        detection = \"Detected \" + str(label) + \" at: \" + detection_time\n",
    "        \n",
    "    # Writing the detection event into the file in txtPath location\n",
    "    with open(txtPath, 'a') as f:\n",
    "        f.write(detection)\n",
    "        f.write('\\n')\n",
    "        \n",
    "# Function create frame that follow the object movement\n",
    "def drawing_tracking(frame_bgr, contour_box, scores, index, tracking, fps):\n",
    "    \n",
    "    # Variable for ain boundries\n",
    "    shift_left = int(WIDTH/5) +10\n",
    "    shift_down = int(HEIGH -FIRST_ROW)\n",
    "    \n",
    "    # Define the prediction time for fragment\n",
    "    model_prediction_time = (end - start)\n",
    "    \n",
    "    # Get the coordinates of the rectangle around the object\n",
    "    (x_min, y_min, box_width, box_height) = [int(a) for a in contour_box]\n",
    "    \n",
    "    # Getting current center coordinates of the bounding box\n",
    "    center = (int(x_min + box_width / 2), int(y_min + box_height / 2))\n",
    "\n",
    "    # Adding current point to the queue\n",
    "    points.appendleft(center)                        \n",
    "        \n",
    "    # Creating image with black background\n",
    "    track_frame = np.zeros(frame_bgr.shape, np.uint8)\n",
    "\n",
    "    # Changing background to Black color\n",
    "    track_frame[:, :, 0] = 0\n",
    "    track_frame[:, :, 1] = 0\n",
    "    track_frame[:, :, 2] = 0\n",
    "\n",
    "    # Visualizing tracker line\n",
    "    for i in range(1, len(points)):\n",
    "        \n",
    "        # If no points collected yet\n",
    "        if points[i - 1] is None or points[i] is None:\n",
    "            continue\n",
    "\n",
    "        # Draw the line between points\n",
    "        cv2.line(track_frame, points[i - 1], points[i], (50, 200, 50), 2)\n",
    "\n",
    "    # Adding text with center coordinates of the bounding box\n",
    "    cv2.putText(track_frame, 'X: {0}'.format(center[0]), (FIRST_COL, FIRST_ROW +5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*4, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(track_frame, 'Y: {0}'.format(center[1]), (FIRST_COL, FIRST_ROW + 35),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*4, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Adding text with time spent for 2D convolution for current frame\n",
    "    cv2.putText(track_frame, 'Time : ' + '{0:.3f}'.format(model_prediction_time), (FIRST_COL, HEIGH -40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*3, (255, 255, 255), 2, cv2.LINE_AA)  \n",
    "\n",
    "    # Adding text with score of convolution for current frame\n",
    "    cv2.putText(track_frame, 'Score : ' + '{0:.3f}'.format(scores[0][index]), (FIRST_COL, HEIGH -15),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*3, (255, 255, 255), 2, cv2.LINE_AA)  \n",
    "\n",
    "    # Adding text with current label on the frame\n",
    "    cv2.putText(track_frame, \"FPS: \" + str(fps), (3*shift_left -2, HEIGH -FIRST_ROW), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, FONT_SIZE*4, (255, 255, 255), 3, cv2.LINE_AA)\n",
    "       \n",
    "    # If Tracking is on - put text on frame\n",
    "    if tracking:\n",
    "        \n",
    "        # Adding text with tracking status on the frame\n",
    "        cv2.putText(track_frame, 'Tracking ', (WIDTH -130, FIRST_ROW + 15), \n",
    "                    cv2.FONT_HERSHEY_TRIPLEX, FONT_SIZE*3, (50, 200, 50), 1, cv2.LINE_AA)\n",
    "        \n",
    "    # Delete the \"Tracking\" alert from the screen    \n",
    "    if not tracking:\n",
    "\n",
    "        track_frame[WIDTH -135:, 0:FIRST_ROW + 15, 0] = 0\n",
    "        track_frame[WIDTH -135:, 0:FIRST_ROW + 15, 1] = 0\n",
    "        track_frame[WIDTH -135:, 0:FIRST_ROW + 15, 2] = 0\n",
    "        \n",
    "        track_frame[center[1], center[0], 0] = 0\n",
    "        track_frame[center[1], center[0], 1] = 0\n",
    "        track_frame[center[1], center[0], 2] = 0\n",
    "        \n",
    "        points.clear()\n",
    "\n",
    "    return track_frame\n",
    "  \n",
    "# Function create the information frame in the main window\n",
    "def info_frame(inf_frame, fps, counter_images_reading, counter_images_processing, counter_images_tracking, time, label):\n",
    "    \n",
    "    # Variable for ain boundries\n",
    "    shift_left = int(WIDTH/5) +10\n",
    "    shift_down = int(HEIGH -FIRST_ROW)\n",
    "        \n",
    "    # Define the font size as 2*FONT_SIZE=(HEIGHT/1000)\n",
    "    font_size = (FONT_SIZE*2)\n",
    "    \n",
    "    # Initilize frame with white background\n",
    "    inf_frame = empty_frame(inf_frame)\n",
    "    \n",
    "    # Variable hold amount images processing in percentage\n",
    "    processing_percentage = str(int((counter_images_processing * 100) / counter_images_reading))+\"%\"\n",
    "    tracking_percentage = str(int((counter_images_tracking * 100) / counter_images_reading))+\"%\"\n",
    "    \n",
    "    # Adding text with right time for current frame\n",
    "    cv2.putText(inf_frame, strftime(\"%H:%M:%S\"), (WIDTH -shift_left, shift_down), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "    \n",
    "    # Adding text with right date for current frame\n",
    "    cv2.putText(inf_frame, strftime(\"%d/%m/%Y\"), (3*shift_left+20, shift_down +20), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)  \n",
    "    \n",
    "    # Adding text with Model name of the app\n",
    "    cv2.putText(inf_frame, \"Model: ResNet50\", (FIRST_COL, FIRST_ROW), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)  \n",
    "    \n",
    "    # Adding text with the training datset of the app\n",
    "    cv2.putText(inf_frame, \"Dataset: Cifar10\", (FIRST_COL, FIRST_ROW +ROWS_SPACE), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)  \n",
    "    \n",
    "    # Adding text with Camera FPS for current frame\n",
    "    cv2.putText(inf_frame, 'Camera FPS: ' + '{0:.0f}'.format(fps), (FIRST_COL, FIRST_ROW + (ROWS_SPACE*2)), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA)  \n",
    "    \n",
    "    # Adding text with reading frames counter for current frame\n",
    "    cv2.putText(inf_frame, 'Reading Frames: ' + '{0:.0f}'.format(counter_images_reading),(FIRST_COL, FIRST_ROW +(ROWS_SPACE*3)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA) \n",
    "    \n",
    "    # Adding text with time spent for 2D convolution for current frame\n",
    "    cv2.putText(inf_frame, 'Tracking Images: ' + tracking_percentage, (FIRST_COL, FIRST_ROW +(ROWS_SPACE*4)), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA) \n",
    "    \n",
    "    # Adding text with processing images counter for current frame\n",
    "    cv2.putText(inf_frame, 'Processing Images: ' + processing_percentage, (FIRST_COL, FIRST_ROW +(ROWS_SPACE*5)), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 0), 1, cv2.LINE_AA) \n",
    "        \n",
    "    # Define the next line where the detections text will start\n",
    "    end_text_line = (FIRST_ROW +(ROWS_SPACE*6)) +5\n",
    "    \n",
    "    # Function put text of all detections events on the frame\n",
    "    plotting_detections(inf_frame, label, time, end_text_line)\n",
    "    \n",
    "    return inf_frame\n",
    "\n",
    "# Function coninue info_frame Function and put text of detection in info frame\n",
    "def plotting_detections(inf_frame, label, time, start_text_line):\n",
    "\n",
    "    # Define the font size as 2*FONT_SIZE=(HEIGHT/1000)\n",
    "    font_size = (FONT_SIZE*2)\n",
    "    \n",
    "    # Find max label string length\n",
    "    max_len = 0\n",
    "    for i in range(len(detected_labels)):\n",
    "        if( max_len < len(detected_labels[i])):\n",
    "            max_len = len(detected_labels[i])\n",
    "                     \n",
    "    # Checking if there is detection\n",
    "    if 0 < len(label): \n",
    "        \n",
    "        # Order all line to the same length\n",
    "        if len(label) < max_len:\n",
    "            label = str(label) + (max_len - len(label))*\" \"\n",
    "            \n",
    "        detection = \"Detected \" + str(label) + \" at: \" + time\n",
    "\n",
    "        # Insert first element to log array\n",
    "        if len(log) == 0:\n",
    "            log.append(detection)\n",
    "        else:\n",
    "            # Check there ara different time of detection event\n",
    "            if detection != log[-1]:\n",
    "                  \n",
    "                # Calculate the how many minutes passed from the last detection    \n",
    "                time_past_in_minutes = ditections_timer(time, log[-1])\n",
    "                log.append(detection)\n",
    "    \n",
    "    # Line number we start write objects we detected\n",
    "    line = start_text_line\n",
    "    \n",
    "    # Define the end line bounderies\n",
    "    end_line = (HEIGH - FIRST_ROW -20)\n",
    "    \n",
    "    # Scan all the detections object to plot them on the frame\n",
    "    for i in range(len(log)):\n",
    "        \n",
    "        # Variable represent the detection event as a string\n",
    "        event = log[i]\n",
    "        \n",
    "        # Check frames boundaries including text\n",
    "        if end_line <= line:\n",
    "            \n",
    "            # Delete old detection by set pixels to while and initilize line to start\n",
    "            inf_frame[start_text_line -20:end_line,:] = 255\n",
    "            \n",
    "            # Back to line 130\n",
    "            line = start_text_line\n",
    "        \n",
    "        # Adding text with DETECTION EVENT for current frame\n",
    "        cv2.putText(inf_frame, event, (10, line), cv2.FONT_HERSHEY_SIMPLEX, font_size, (0, 0, 255), 1, cv2.LINE_AA) \n",
    "        \n",
    "        # skip to the next line\n",
    "        line += int(HEIGH/10) - 5  \n",
    "\n",
    "# Function get 3 frames and collect them to 1 frame \n",
    "def collaction_frames(left_frame, mid_frame, right_frame):\n",
    "\n",
    "    # Insert all frames to array for scan it\n",
    "    frames = [left_frame, mid_frame, right_frame]\n",
    "\n",
    "    # Change all frames to 3 chanels    \n",
    "    for i in range(len(frames)):\n",
    "        \n",
    "        # Check if frames[i][3] is exis\n",
    "        if len(frames[i].shape) < 3:\n",
    "            \n",
    "            # Adding 3-Dimension to the image\n",
    "            frames[i] = frames[i][:,:,np.newaxis]\n",
    "        \n",
    "        # Find frames that not 3 channels\n",
    "        if frames[i].shape[2] != 3:\n",
    "             \n",
    "            # Function expand mask's dimension from 1 to 3 odimensions    \n",
    "            frames[i] = expands_dimensions(frames[i])\n",
    "                \n",
    "    # Define frames in the right order\n",
    "    left_frame = frames[0]\n",
    "    mid_frame = frames[1]\n",
    "    right_frame = frames[2]\n",
    "    \n",
    "    # Create one window that contain: frame, tracking, mask\n",
    "    collaction_frame = np.hstack((left_frame, mid_frame, right_frame))\n",
    "    \n",
    "    return collaction_frame\n",
    "\n",
    "# Function Print every detection event \n",
    "def print_detections():\n",
    "    \n",
    "    # Print every detection event     \n",
    "    print(\"\\n###################### Detection Events #########################\\n\")\n",
    "    for i in range(len(log)):\n",
    "        print(log[i])\n",
    "    print(\"\\n###################### Detection Events #########################\\n\")\n",
    "\n",
    "# Function print all the app's information on the screen\n",
    "def print_info(counter_frames_reading, counter_frames_tracking, counter_success_tracking, \n",
    "               counter_frames_processing, counter_images_processing, counter_frames_predictions, \n",
    "               counter_birds_predictions, counter_fail_predictions, counter_frames_not_processing):\n",
    "    \n",
    "    print(\"\\n###################### Application Information ##################\\n\")\n",
    "    print(\"Frame HEIGH: \", HEIGH, \"\\nFrame WIDTH: \", WIDTH)\n",
    "    print(\"Model Input hight: \", hight, \"\\nModel Input width: \", width)\n",
    "    print(\"mask_number: \", mask_number, \"\\ntracking_check: \", tracking_check)\n",
    "    print(\"MIN_OBJECT_AREA: \", MIN_OBJECT_AREA, \"\\nMAX_OBJECT_AREA: \", MAX_OBJECT_AREA)\n",
    "    print(\" \")\n",
    "    print(\"counter_frames_reading \", counter_frames_reading)\n",
    "    print(\"counter_frames_tracking \", counter_frames_tracking)\n",
    "    print(\"counter_success_tracking \", counter_success_tracking)\n",
    "    print(\"counter_frames_processing \", counter_frames_processing)\n",
    "    print(\"counter_images_processing \", counter_images_processing)\n",
    "    print(\"counter_frames_predictons \", counter_frames_predictions)\n",
    "    print(\"counter_birds_predictions \", counter_birds_predictions)\n",
    "    print(\"counter_frames_fail_predictions \", counter_fail_predictions)                                \n",
    "    print(\"counter_frames_not_processing\", counter_frames_not_processing)\n",
    "    print(\" \")\n",
    "    print(\"Tracking Frames: \", str(int((counter_success_tracking * 100) / counter_frames_reading))+\"%\")\n",
    "    print(\"Processing Frames: \", str(int((counter_frames_processing * 100) / counter_frames_reading))+\"%\")\n",
    "    print(\"Right Predictions: \", '{0:.0f}'.format((counter_birds_predictions*100)/counter_frames_predictions)+\"%\")\n",
    "    print(\" \")\n",
    "    print(\"###################### Application Information ##################\")\n",
    "\n",
    "# Function writes all the app's information into a text file\n",
    "def write_info_txt(counter_frames_reading, counter_frames_tracking, counter_success_tracking, \n",
    "                   counter_frames_processing, counter_images_processing, counter_frames_predictions, \n",
    "                   counter_birds_predictions, counter_fail_predictions, counter_frames_not_processing):\n",
    "    \n",
    "    # Stores all the application's information in array and then writes them into a file\n",
    "    app_info = []\n",
    "    app_info.append(str(\"\\n###################### Application Information ##################\\n\"))\n",
    "    app_info.append(str(\"counter frames reading: \" + str(counter_frames_reading)))\n",
    "    app_info.append(str(\"counter frames tracking: \" + str(counter_frames_tracking)))\n",
    "    app_info.append(str(\"counter success tracking: \" + str(counter_success_tracking)))\n",
    "    app_info.append(str(\"counter frames processing: \" + str(counter_frames_processing)))\n",
    "    app_info.append(str(\"counter images processing: \" + str(counter_images_processing)))\n",
    "    app_info.append(str(\"counter frames predictons: \" + str(counter_frames_predictions)))\n",
    "    app_info.append(str(\"counter birds predictions: \" + str(counter_birds_predictions)))\n",
    "    app_info.append(str(\"counter fail predictions: \" + str(counter_fail_predictions)))\n",
    "    app_info.append(str(\"counter frames not processing: \" + str(counter_frames_not_processing)))\n",
    "    app_info.append(str(\" \"))\n",
    "    app_info.append(str(\"Tracking Frames: \"+ str(int((counter_success_tracking * 100) / counter_frames_reading))+\"%\"))\n",
    "    app_info.append(str(\"Processing Frames: \"+str(int((counter_frames_processing * 100) / counter_frames_reading))+\"%\"))\n",
    "    app_info.append(str(\"Right Predictions: \"+'{0:.0f}'.format((counter_birds_predictions*100)/counter_frames_predictions)+\"%\"))\n",
    "    app_info.append(str(\" \"))\n",
    "    app_info.append(str(\"############################## END ################################\\n\\n\"))\n",
    "\n",
    "    # Writing all application's information into a text file\n",
    "    with open(txtPath, 'a') as f:\n",
    "        f.write('\\n'.join(app_info))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3669412",
   "metadata": {},
   "source": [
    "### Define Application Variables ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06b925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable hold the prediction label\n",
    "label = \"\"\n",
    "\n",
    "# Variable hold time of last detection\n",
    "last_time_detection = \"\"\n",
    "\n",
    "# Variable for first time detection aim as time right now\n",
    "detection_time = strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "\n",
    "# Initialize FPS variable\n",
    "FPS = 0\n",
    "\n",
    "# Variable hold all the detections events\n",
    "log = []\n",
    "\n",
    "# Save all the labels we detected\n",
    "detected_labels = []\n",
    "\n",
    "# Initialize stop tracking variable\n",
    "Stop_Tracking = False\n",
    "\n",
    "# Initialize TRACKING variables\n",
    "tracking = False\n",
    "\n",
    "# Initialize FIRST variables\n",
    "first = True\n",
    "\n",
    "# Restart timer for FPS\n",
    "fps_start = timer() \n",
    "fps_end = timer()\n",
    "\n",
    "# Restart prediction's timer\n",
    "start = timer()\n",
    "end = timer()\n",
    "\n",
    "# Initialize counters variables\n",
    "counter_predictions = 0\n",
    "counter_frames_saver = 0\n",
    "counter_fail_tracking = 0\n",
    "counter_frames_reading = 0\n",
    "counter_frames_tracking = 0\n",
    "counter_frames_processing = 0\n",
    "counter_images_processing = 0\n",
    "counter_success_tracking = 0\n",
    "counter_fail_predictions = 0\n",
    "counter_frames_per_second = 0\n",
    "counter_birds_predictions = 0\n",
    "counter_frames_predictions = 0\n",
    "counter_frames_not_processing = 0\n",
    "\n",
    "last_boundries = [0, 0, 0, 0] \n",
    "\n",
    "# initialize variables\n",
    "scores = np.array([(0,float(0))])\n",
    "index = 0\n",
    "\n",
    "# Capturing first frame from camera\n",
    "ret, frame_bgr = camera.read()\n",
    "\n",
    "# Check to camera\\video path\n",
    "if not ret or frame_bgr is None:\n",
    "    print(\"Video path not found. \")\n",
    "else: \n",
    "    # Resize the main frame to (HEIGH, WIDTH)\n",
    "    frame_bgr = cv2.resize(frame_bgr, (WIDTH, HEIGH))\n",
    "\n",
    "    # Create frame for previos frame_bgr\n",
    "    last_frame = duplicate_frame(frame_bgr)\n",
    "\n",
    "    # Define the frame for the first frames in the main window\n",
    "    scores_frame = empty_frame(frame_bgr)\n",
    "    tracking_frame = empty_frame(frame_bgr)\n",
    "    cut_fragment_bgr_frame = empty_frame(frame_bgr)\n",
    "    demo_frame = empty_frame(frame_bgr)\n",
    "    mask_frame = np.zeros((HEIGH, WIDTH), np.uint8)\n",
    "\n",
    "    # Converting captured frame to GRAY by OpenCV function  \n",
    "    frame_gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35431758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function tracking after objects\n",
    "def objects_tracking(frame_bgr, tracking, counter_frames_tracking, counter_success_tracking):\n",
    "    \n",
    "    # Increase tracking counter\n",
    "    counter_frames_tracking +=1\n",
    "\n",
    "    # Get the bounding box from the frame\n",
    "    (success, contour_box) = tracker.update(frame_bgr)\n",
    "\n",
    "    # Strart\\Keep tracking\n",
    "    if success:   \n",
    "\n",
    "        # Save the last contour box to show last tracking location\n",
    "        last_boundries = contour_box\n",
    "\n",
    "        # Get the coordinates of the rectangle around the object\n",
    "        (x, y, w, h) = [int(a) for a in contour_box]\n",
    "\n",
    "        # Check if coordinates is in the frame boundaries\n",
    "        if 0 < x and x+w < WIDTH and 0 < y and y+h < HEIGH:   \n",
    "\n",
    "            # Cut the fragment from the mask frame\n",
    "            cut_fragment_mask = mask_frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Checking if tracking is still running after the object or not\n",
    "            if((counter_frames_reading%5 == 0 and np.sum(cut_fragment_mask) == 0)):      \n",
    "\n",
    "                # Initializes variable\n",
    "                Stop_Tracking = True\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                # Set tracking status ON\n",
    "                tracking = True\n",
    "\n",
    "                # Function return a frame with the tracking of the cut fragment\n",
    "                tracking_frame = drawing_tracking(frame_bgr, contour_box, scores, index, tracking, FPS +1)      \n",
    "\n",
    "                # Putting text with label on the current BGR frame\n",
    "                cv2.putText(frame_bgr, label, (x - 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)  \n",
    "                   \n",
    "                # Drawing bounding box on the current BGR frame        \n",
    "                cv2.rectangle(frame_bgr, (x,y), (x+w,y+h), (100,255,0), 2)\n",
    "                \n",
    "                # Increase the tracking counter\n",
    "                counter_success_tracking += 1\n",
    "\n",
    "                # Variable to keep tracking\n",
    "                Stop_Tracking = False\n",
    "\n",
    "    # Return BGR frame with rectangle drawing around the objects\n",
    "    return frame_bgr, tracking, counter_frames_tracking, counter_success_tracking, Stop_Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a78a2",
   "metadata": {},
   "source": [
    "### Images Processing and Model Predictions ###\n",
    "\n",
    "Reading frames from camera frame by frame and recognize movments to detect object in the frame. The detected object is cutted from the original frame and resize to the model input shape. The cutted object sent to model and gives us the label prediction with the score. After we detected the object we drawing green rectangle around it and start to tracking after the object. We display to the user one window that divide to six small windows:\n",
    "\n",
    "1. The frame that we read from camera with the rectangle around the object and the label above it.\n",
    "2. Window that contain information of the image processing like model's name, detection event, time, etc'.\n",
    "3. Window that draw a line that present the tracking of the detected object.\n",
    "4. Frame of the object we cutted form the original frame and sent to the model.\n",
    "5. Window that show us graph with all the labels and their score from predicted object.\n",
    "6. Window that display us the mask we use to detect object and movements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8aa56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 899ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "\n",
      "###################### Detection Events #########################\n",
      "\n",
      "Detected bird at: 14/01/2023 15:40:52\n",
      "Detected bird at: 14/01/2023 15:40:55\n",
      "Detected bird at: 14/01/2023 15:41:04\n",
      "Detected bird at: 14/01/2023 15:41:05\n",
      "Detected bird at: 14/01/2023 15:41:06\n",
      "Detected cat  at: 14/01/2023 15:41:06\n",
      "Detected bird at: 14/01/2023 15:41:07\n",
      "Detected bird at: 14/01/2023 15:41:08\n",
      "Detected bird     at: 14/01/2023 15:41:14\n",
      "Detected bird     at: 14/01/2023 15:41:32\n",
      "Detected bird     at: 14/01/2023 15:41:55\n",
      "\n",
      "###################### Detection Events #########################\n",
      "\n",
      "\n",
      "###################### Application Information ##################\n",
      "\n",
      "Frame HEIGH:  250 \n",
      "Frame WIDTH:  400\n",
      "Model Input hight:  224 \n",
      "Model Input width:  224\n",
      "mask_number:  6 \n",
      "tracking_check:  10000\n",
      "MIN_OBJECT_AREA:  1000 \n",
      "MAX_OBJECT_AREA:  8000\n",
      " \n",
      "counter_frames_reading  1312\n",
      "counter_frames_tracking  1287\n",
      "counter_success_tracking  1274\n",
      "counter_frames_processing  16\n",
      "counter_images_processing  16\n",
      "counter_frames_predictons  18\n",
      "counter_birds_predictions  14\n",
      "counter_frames_fail_predictions  4\n",
      "counter_frames_not_processing 9\n",
      " \n",
      "Tracking Frames:  97%\n",
      "Processing Frames:  1%\n",
      "Right Predictions:  78%\n",
      " \n",
      "###################### Application Information ##################\n"
     ]
    }
   ],
   "source": [
    "# Loop reading frame by frame and processing them\n",
    "while True:\n",
    "    \n",
    "    # Capturing frames one-by-one from camera\n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    # If the frame was not retrieved then we break the loop\n",
    "    if not ret or frame is None:\n",
    "        break\n",
    "\n",
    "    # Stopping the timer for FPS\n",
    "    fps_stop = timer()\n",
    "\n",
    "    # Increasing counter\n",
    "    counter_frames_reading += 1\n",
    "    \n",
    "    # Increasing FPS counter\n",
    "    counter_frames_per_second += 1\n",
    "    \n",
    "    # Define the fps of the loop using cv2 function\n",
    "    fps = int(camera.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Resize the main frame to (WIDTH, HEIGH) shape\n",
    "    frame = cv2.resize(frame, (WIDTH, HEIGH))\n",
    "\n",
    "    # Create frame for previos frame_bgr  \n",
    "    frame_bgr = duplicate_frame(frame)\n",
    "\n",
    "    # Apply the frame to forground model\n",
    "    foreground_mask = foregroundModel.apply(frame) \n",
    "                \n",
    "    # Function return one from 7 different kinds of masks according to the mask_number              \n",
    "    mask_frame = mask(frame, last_frame, foreground_mask, mask_number)\n",
    "       \n",
    "    # Checking the sum of pixels values to estimate the background\n",
    "    if(500 < np.sum(mask_frame)):\n",
    "        \n",
    "        # Define last frame (using to detect movments)\n",
    "        last_frame = duplicate_frame(frame_bgr)\n",
    "            \n",
    "        # End of if contours       \n",
    "        if tracking:\n",
    "                       \n",
    "            # Increase tracking counter\n",
    "            counter_frames_tracking +=1\n",
    "            \n",
    "            # Get the bounding box from the frame\n",
    "            (success, contour_box) = tracker.update(frame_bgr)\n",
    "\n",
    "            # Strart\\Keep tracking\n",
    "            if success:   \n",
    "            \n",
    "                # Save the last contour box to show last tracking location\n",
    "                last_boundries = contour_box\n",
    "                                \n",
    "                # Get the coordinates of the rectangle around the object\n",
    "                (x, y, w, h) = [int(a) for a in contour_box]\n",
    "\n",
    "                # Check if coordinates is in the frame boundaries\n",
    "                if 0 < x and x+w < WIDTH and 0 < y and y+h < HEIGH:   \n",
    "                    \n",
    "                    # Cut the fragment from the mask frame\n",
    "                    cut_fragment_mask = mask_frame[y:y+h, x:x+w]\n",
    "                    \n",
    "                    # Checking if tracking is still running after the object or not\n",
    "                    if((counter_frames_reading%5 == 0 and np.sum(cut_fragment_mask) == 0)):      \n",
    "                        \n",
    "                        # Initializes variable\n",
    "                        Stop_Tracking = True\n",
    "                    else:\n",
    "                        # Set tracking status ON\n",
    "                        tracking = True\n",
    "        \n",
    "                        # Function return a frame with the tracking of the cut fragment\n",
    "                        tracking_frame = drawing_tracking(frame_bgr, contour_box, scores, index, tracking, FPS +1)                         \n",
    "\n",
    "                        # Drawing bounding box on the current BGR frame        \n",
    "                        cv2.rectangle(frame_bgr, (x,y), (x+w,y+h), (100,255,0), 2)\n",
    "\n",
    "                        # Putting text with label on the current BGR frame\n",
    "                        cv2.putText(frame_bgr, label, (x - 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)  \n",
    "\n",
    "                        # Increase the tracking counter\n",
    "                        counter_success_tracking += 1\n",
    "                        \n",
    "                        # Variable to keep tracking\n",
    "                        Stop_Tracking = False\n",
    "                    \n",
    "            # Stop the tracking after the object    \n",
    "            if Stop_Tracking:\n",
    "                \n",
    "                # Clear all the coordinates from the Deque points\n",
    "                points.clear()   \n",
    "\n",
    "                # Initialize Variables to predict the next fragment\n",
    "                first, tracking = True, False\n",
    "\n",
    "                # Function return a frame with the tracking of the cut fragment\n",
    "                tracking_frame = drawing_tracking(frame_bgr, last_boundries, scores, index, tracking, FPS +1)  \n",
    "                \n",
    "            # Initializes variable\n",
    "            Stop_Tracking = True\n",
    "                \n",
    "        # End of if tracking\n",
    "        else:     \n",
    "            \n",
    "            # Increase tracking counter\n",
    "            counter_frames_processing +=1\n",
    "            \n",
    "            # Find contsours from the mask to detect objects\n",
    "            contours, _ = cv2.findContours(mask_frame, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "            # version 2 of code only draw large change\n",
    "            if contours and not tracking:\n",
    "\n",
    "                # Variable for first time recognize a bird\n",
    "                first = True\n",
    "\n",
    "                # Variable hold index of contour in contours array\n",
    "                contour_index = 0\n",
    "\n",
    "                # Sorted the contours and define the larger first\n",
    "                contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "                # Scan all the contours we found in the frame\n",
    "                for contour in contours:\n",
    "                    \n",
    "                    # Return square area of the given contour\n",
    "                    contour_area = abs(cont_area(contour, contour_index))\n",
    "                    \n",
    "                    # Avoid large objects ( max = 10,000)\n",
    "                    if contour_area < MAX_OBJECT_AREA:\n",
    "                        \n",
    "                        # Avoid small objects ( min = 1,000 )\n",
    "                        if MIN_OBJECT_AREA < contour_area:\n",
    "\n",
    "                            # Function return cut bgr fragments (will use as input to the model)\n",
    "                            cut_fragment_bgr_frame = cut_fragment(frame, contours, contour_index)\n",
    "\n",
    "                            # Predic model output every two frames\n",
    "                            if counter_frames_reading%2 >= 0:\n",
    "                                \n",
    "                                # Increase prediction counter\n",
    "                                counter_frames_predictions += 1\n",
    "                                \n",
    "                                # Current time of detection object\n",
    "                                detection_time = strftime(\"%d/%m/%Y %H:%M:%S\")  \n",
    "\n",
    "                                \"############################### Start of Model Prediction ###########################\"\n",
    "                                # Measuring classification time\n",
    "                                start = timer()\n",
    "                                    \n",
    "                                # Function return all scores of model predictions\n",
    "                                scores = prediction_model(cut_fragment_bgr_frame)       \n",
    "\n",
    "                                # End of Measuring classification time\n",
    "                                end = timer()\n",
    "                                \"############################### End of Model Prediction #############################\"\n",
    "                        \n",
    "                                # Finds the lables array index by the max score index of model prediction\n",
    "                                index = np.argmax(scores)\n",
    "\n",
    "                                # Define the label for the cut_fragment from labels array\n",
    "                                label = labels[index]\n",
    "                                \n",
    "                                # Function return a bgr frame with rectangle around the cut fragment\n",
    "                                frame_bgr = drawing_rectangle(frame_bgr, label, contours, contour_index)  \n",
    "                            \n",
    "                                # Save detection event in date-time format\n",
    "                                save_detection_event(frame_bgr, detection_time, label)\n",
    "\n",
    "                                # Function return a frame with all the labels scores     \n",
    "                                scores_frame = bar_chart(scores[0], labels)  \n",
    "                            \n",
    "                            # Save the label we detected\n",
    "                            if label not in detected_labels:\n",
    "                                detected_labels.append(label)          \n",
    "                                        \n",
    "                            # Start traking after the detected bird\n",
    "                            if label == \"bird\":\n",
    "                                                               \n",
    "                                # Clear all the coordinates from the Deque points\n",
    "                                points.clear()  \n",
    "                                \n",
    "                                winsound.PlaySound(soundPath, winsound.SND_FILENAME)\n",
    "\n",
    "                                # Increasing Birds prediction counter\n",
    "                                counter_birds_predictions += 1\n",
    "                                \n",
    "                                # First time detection (before tracking)\n",
    "                                if first:\n",
    "                                    \n",
    "                                    # Get an approximate rectangle coordinates\n",
    "                                    (x_min, y_min, box_width, box_height) = cv2.boundingRect(contours[contour_index])\n",
    "                                        \n",
    "                                    rectangle = np.array([x_min, y_min, box_width, box_height])\n",
    "                                    tracker.init(frame_bgr, rectangle)\n",
    "                                    tracking = True\n",
    "                                    first = False\n",
    "                                    break                                   \n",
    "                                    \n",
    "                            # If not detected birds        \n",
    "                            else: \n",
    "                                # Increasing fail predictions counter\n",
    "                                counter_fail_predictions = counter_frames_predictions - counter_birds_predictions\n",
    "                                                        \n",
    "                        # End of MAX CONTOUR < contour_area    \n",
    "                        else:\n",
    "                            # Skip on contours area small than MIN CONTOUR\n",
    "                            break                           \n",
    "                        # End of if\\else MIN_OBJECT_AREA < contour_area                         \n",
    "                    # End of if\\else contour_area < MAX_OBJECT_AREA  \n",
    "                            \n",
    "                    # Increase the index of contour in the contours array\n",
    "                    contour_index += 1      \n",
    "                    \n",
    "                # End of for contour loop  \n",
    "                \n",
    "                # Increasing processing counter\n",
    "                counter_images_processing += 1\n",
    "                \n",
    "            # End of if countrs and not tracking\n",
    "        # End of if\\else tracking\n",
    "    # if(500 < np.sum(mask_frame))\n",
    "               \n",
    "        # Function return frame that contain the app stats and informations\n",
    "        inf_frame = info_frame(demo_frame, fps, counter_frames_reading, counter_frames_processing, counter_frames_tracking, \n",
    "                               detection_time, label)\n",
    "\n",
    "        # Function return one window that contain (frame_bgr, track_frame, mask_frame)\n",
    "        upper_window = collaction_frames(frame_bgr, inf_frame, tracking_frame)\n",
    "        \n",
    "        # Function return one window that contain (cut_fragment_bgr_frame, scores_frame, mask_frame)\n",
    "        lower_window = collaction_frames(cut_fragment_bgr_frame, scores_frame, mask_frame)\n",
    "        \n",
    "        # Create one window that contain: upper_window and lower_window\n",
    "        main_window = np.vstack((upper_window, lower_window))\n",
    "        \n",
    "        # Plotting all the frames in one window\n",
    "        cv2.imshow(\"Main_Window\", main_window) \n",
    "                \n",
    "    # End of if(500 < np.sum(mask_frame)):\n",
    "    else:\n",
    "        # Increasing the unprocessing counter\n",
    "        counter_frames_not_processing += 1\n",
    "                            \n",
    "        # Function return a frame with the tracking of the cut fragment\n",
    "        tracking_frame = drawing_tracking(frame_bgr, last_boundries, scores, index, tracking, FPS)  \n",
    "        \n",
    "        # Function return frame that contain the app stats and informations\n",
    "        inf_frame = info_frame(demo_frame, fps, counter_frames_reading, counter_frames_processing, counter_frames_tracking, \n",
    "                                detection_time, label)\n",
    "        \n",
    "        # Function return one window that contain (frame_bgr, track_frame, mask_frame)\n",
    "        upper_window = collaction_frames(frame_bgr, inf_frame, tracking_frame)\n",
    "        \n",
    "        # Function return one window that contain (cut_fragment_bgr_frame, scores_frame, mask_frame)\n",
    "        lower_window = collaction_frames(cut_fragment_bgr_frame, scores_frame, mask_frame)\n",
    "        \n",
    "        # Create one window that contain: upper_window and lower_window\n",
    "        main_window = np.vstack((upper_window, lower_window))\n",
    "    \n",
    "        # Plotting all the frames in one window\n",
    "        cv2.imshow(\"Main_Window\", main_window)     \n",
    "    \n",
    "    # Print FPS every 1 second\n",
    "    if 1.0 <= fps_stop - fps_start:\n",
    "        \n",
    "        # Define FPS\n",
    "        FPS = counter_frames_per_second\n",
    "        \n",
    "        # Reset FPS counter\n",
    "        counter_frames_per_second = 0\n",
    "\n",
    "        # Restart timer for FPS\n",
    "        fps_start = timer()    \n",
    "                \n",
    "    # Function waits for key to be pressed    \n",
    "    key = cv2.waitKey(1) % 256\n",
    "\n",
    "    # If 'q' key is pressed then quit from app\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "     \n",
    "    # if 'i' key is pressed then print all the pixels values\n",
    "    if key == ord('i'):\n",
    "        \n",
    "        # Print the pixel's values sum and last contour area\n",
    "        print(\"Sum: \", np.sum(mask_frame))\n",
    "        print(\"Area: \", contour_area)\n",
    "     \n",
    "    # If 'n' key is pressed then release the tracking and move on\n",
    "    if key == ord('n'):\n",
    "        \n",
    "        # Clear all the coordinates from the Deque points\n",
    "        points.clear()   \n",
    "\n",
    "        # Initialize Variables to predict the next fragment\n",
    "        first, tracking = True, False\n",
    "                    \n",
    "        # Function return a frame with the tracking of the cut fragment\n",
    "        tracking_frame = drawing_tracking(frame_bgr, last_boundries, scores, index, tracking, FPS)  \n",
    "\n",
    "# Releasing camera\n",
    "camera.release()\n",
    "\n",
    "# Destroying all opened OpenCV windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print every detection event \n",
    "print_detections()\n",
    "\n",
    "# Print all the app's information \n",
    "print_info(counter_frames_reading, counter_frames_tracking, counter_success_tracking, counter_frames_processing, \n",
    "           counter_images_processing, counter_frames_predictions, counter_birds_predictions, counter_fail_predictions, \n",
    "           counter_frames_not_processing)\n",
    "\n",
    "# Function write all the app's information into a txt file\n",
    "write_info_txt(counter_frames_reading, counter_frames_tracking, counter_success_tracking, counter_frames_processing, \n",
    "               counter_images_processing, counter_frames_predictions, counter_birds_predictions, counter_fail_predictions, \n",
    "               counter_frames_not_processing)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
